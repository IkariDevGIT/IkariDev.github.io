<!-- Add readability mode(other font) -->

<div>
    <div class="blogbox" id="blogid-6">
        <h3>Noromaid model series</h3>
        <i class="timestamp">19.12.2023 - 22:51 [GMT+2]</i><div style="padding-top: 20px"></div>
        <a href="https://discord.gg/w6C8mys35E">NeverSleep Discord</a>
        <p>Name comes from:<br>Noro = <span style="color: red;">No</span>_<span style="color: red;">ro</span>bots(dataset),<br>Maid = <span style="color: red;">M</span>inerva <span style="color: red;">AI</span> <span style="color: red;">D</span>ataset</p>
        <p>MinervaAI Dataset referring to: Aesir</p>
        <br>
        <div id="line"></div>
        <h4 id="Noromaid-0.1.1">Noromaid 0.1.1</h4>
        <ul style="padding-left: 10px;">
            <p>- <a href="https://huggingface.co/NeverSleep/Noromaid-13b-v0.1.1">Noromaid-13b-v0.1.1</a></p>
            <p>- <a href="https://huggingface.co/NeverSleep/Noromaid-7b-v0.1.1">Noromaid-7b-v0.1.1</a></p>
            <p>- <a href="https://huggingface.co/NeverSleep/Noromaid-20b-v0.1.1">Noromaid-20b-v0.1.1</a></p>
        </ul>
        <p>Yet another merge with undi, came out 16.11.2023, main part of it a dataset by <a href="https://huggingface.co/MinervaAI">MinervaAI</a> named aesir, other part is a <a href="https://huggingface.co/datasets/Doctor-Shotgun/no-robots-sharegpt">modified version of No_Robots</a>.</p>
        <br>
        <div id="line"></div>
        <h4 id="Noromaid-0.2">Noromaid 0.2  [ EXPERIMENTAL ]</h4>
        <ul style="padding-left: 10px;">
            <p>- <a href="https://huggingface.co/NeverSleep/Noromaid-13b-v0.2">Noromaid-13b-v0.2</a></p>
            <p>- <a href="https://huggingface.co/NeverSleep/Noromaid-7b-v0.2">Noromaid-7b-v0.2</a></p>
        </ul>
        <p>As seen in the title, this model is pretty experimental!</p>
        <p>This time we used <a href="https://github.com/Gryphe/MergeMonster">MergeMonster</a>, with many bad_phrases. We also used another <a href="https://huggingface.co/MinervaAI">MinervaAI</a> dataset.</p>
        <p>The 13b was published on 16.12.2023, the 7b was released on 21.12.2023, further B's will be released when we get more feedback</p>
        <br>
        <div id="line"></div>
        <h4 id="Noromaid-0.3">Noromaid 0.3</h4>
        <ul style="padding-left: 10px;">
            <p>- <a href="https://huggingface.co/NeverSleep/Noromaid-13b-v0.3">Noromaid-13b-v0.3</a></p>
        </ul>
        <p>Noromaid 0.3 has: new datasets, normal alpaca Instruct(with instruction, input, and response header) instead of our modified alpaca.</p>
        <p>It is a complete retrain based on the same model as 0.1.1 and 0.1.</p>
        <br>

        <div id="line"></div>
        <h4 id="Noromaid-0.4">Noromaid 0.4</h4>
        <ul style="padding-left: 10px;">
            <p>- <a href="https://huggingface.co/NeverSleepHistorical/Noromaid-7B-0.4">Noromaid-7b-v0.4</a></p>
            <p>- <a href="https://huggingface.co/NeverSleep/Noromaid-7B-0.4-DPO">Noromaid-7b-v0.4-DPO</a></p>
        </ul>
        <p>Noromaid 7b v0.4 is a fully finetuned 7b Mistral trained on multiple RP dataset, modified by our own hand, and redone entirely from Alpaca to Chatml. The switch from Alpaca to Chatml and the addition of a new dataset from Aesir made it even better than 0.3.</p>
        <p>Noromaid 7b v0.4 DPO is the same as Noromaid 0.4 7b. Additionally, it was trained on top with 3 DPO datasets including the Intel DPO for reasoning, and some uncensoring.</p>
        <br>

        <div id="line"></div>
        <h2>Spin-Off's</h2>
        <ul style="padding-left: 25px;">
            <p>(standalone)</p>
            <ul>
                <p id="mixtral-base">Mixtral</p>
                <ul style="padding-left: 25px;">
                    <p id="Noromaid-v0.1-mixtral-8x7b-v1">- <a href="https://huggingface.co/NeverSleepHistorical/Noromaid-v0.1-mixtral-8x7b-v1">Noromaid-v0.1-mixtral-8x7b-v1</a></p>
                    <p style="padding-left: 25px;">Noromaid-v0.1-mixtral-8x7b-v1 is a fine-tune of Mixtral 8x7b, trained on various RP datasets including LimaRP and Aesir, ToxicDPO without warning for decensoring and norobots, rewritten to use a modified Alpaca prompting to be on a par with ChatML or other conversational formats. This version contains 3x Alpaca modified datasets (for the RP one) and 2 shareGPT datasets. LimaRP token length, input, and output size got wiped out, and were separated into chunks of +8k context conversation (this is the maximum LimaRP can offer).</p>
                
                    <p id="Noromaid-v0.1-mixtral-8x7b-v2">- <a href="https://huggingface.co/NeverSleepHistorical/Noromaid-v0.1-mixtral-8x7b-v2">Noromaid-v0.1-mixtral-8x7b-v2</a></p>
                    <p style="padding-left: 25px;">Noromaid-v0.1-mixtral-8x7b-v2 is a fine-tune of Mixtral 8x7b, trained on various RP datasets including LimaRP and Aesir, ToxicDPO without warning for decensoring and norobots, rewritten to use a modified Alpaca prompting to be on a par with ChatML or other conversational formats. This version contains 3x Alpaca modified datasets (for the RP one) and 2 shareGPT datasets. On this v2, LimaRP got fixed further, Axolotl received a monkey patch to actually reformat shareGPT to our modified Alpaca prompting.</p>
                
                    <p id="Noromaid-v0.1-mixtral-8x7b-v3">- <a href="https://huggingface.co/NeverSleep/Noromaid-v0.1-mixtral-8x7b-v3">Noromaid-v0.1-mixtral-8x7b-v3</a></p>
                    <p style="padding-left: 25px;">Noromaid-v0.1-mixtral-8x7b-v3 is a fine-tune of Mixtral 8x7b, trained on various RP datasets including LimaRP and Aesir, ToxicDPO without warning for decensoring and norobots, rewritten to use a modified Alpaca prompting to be on a par with ChatML or other conversational formats. This version contains 5x Alpaca modified datasets. On this v3, all the datasets got trained on the "completion" method of Axolotl, with all the datasets being completely rewritten to be in Alpaca modified format. More than 2600 Wikipedia references got cleaned up of Norobots dataset.</p>
                    
                    <p id="Noromaid-v0.1-mixtral-8x7b-Instruct-v3">- <a href="https://huggingface.co/NeverSleep/Noromaid-v0.1-mixtral-8x7b-Instruct-v3">Noromaid-v0.1-mixtral-8x7b-Instruct-v3</a></p>
                    <p style="padding-left: 25px;">Noromaid-v0.1-mixtral-8x7b-Instruct-v3 is made with a LoRA done on base Mixtral 8x7b. It's the same data than Noromaid-v0.1-mixtral-8x7b-v3, but applied on the Instruct model. Fine-tuning on base and applying on Instruct seem to give better result for our usage: RP/ERP. Stay tuned for more information!</p>

                    <br>
                <p>Noromaid-v0.1-mixtral-8x7b series info: v1 and v2 were each trained for 2 epochs, totaling to each 8 hours on Axolotl. v3, on the other hand, underwent 3 epochs and was trained for 12 hours, bringing the cumulative training time to 28 hours on a single A100 80GB GPU.</p>
                </ul>
                <p id="Miqu-base">Miqu - <a href="https://huggingface.co/collections/NeverSleep/miqumaid-65c3d5e0fd15420346adc906">[HF Collection]</a></p>
                <ul style="padding-left: 25px;">
                    <p id="MiquMaid-v1-70B">- <a href="https://huggingface.co/NeverSleep/MiquMaid-v1-70B">MiquMaid-v1-70B</a></p>
                    <p style="padding-left: 25px;">Quick train to see if <a href="https://huggingface.co/152334H/miqu-1-70b-sf">miqu</a> finetuned results in good models.</p>

                    <p id="MiquMaid-v2-70B-alpha-GGUF">- <a href="https://huggingface.co/NeverSleepHistorical/MiquMaid-v2-70B-alpha-GGUF">MiquMaid-v2-70B-alpha-GGUF (GGUF only)</a></p>
                    <p style="padding-left: 25px;">MiquMaid v2-alpha is trained on 1 epoch for 18h running on 2xA100 80GB. Trained on <a href="https://huggingface.co/152334H/miqu-1-70b-sf">miqu</a>. <span style="color: red;">Deprecated!</span></p>


                    <p id="MiquMaid-v2-70B">- <a href="https://huggingface.co/NeverSleep/MiquMaid-v2-70B">MiquMaid-v2-70B</a> / <a href="https://huggingface.co/NeverSleep/MiquMaid-v2-70B-DPO">[DPO]</a></p>
                    <p style="padding-left: 25px;">MiquMaid-v1-70B was our first 70B model, based on the leaked Mistral Medium model. V1 used Aesir datasets where V2 make the return of Norobots and some uncensoring data in it to make it even more unethical in RP.
                    </br>
                    V1 was already compliant to a lot of things, even on some prompt Mistral Medium would refuse 100%, because it is HIGHLY aligned. This V2 let you prompt even more unethical and unhinged RP. Not using any RP format show a rate of refusal really lower than Mistral Medium too!
                    </br>
                    On top of that, a DPO train, using the same data that was used in the OG Finetune for better performance made it even better, write better, and be even more uncensored. The model lose some points in benchmark, but the tradeback for really good RP and less repetition was worth it.</p>
                    
                    <p id="MiquMaid-v2-2x70B">- <a href="https://huggingface.co/NeverSleep/MiquMaid-v2-2x70B">MiquMaid-v2-70B</a> / <a href="https://huggingface.co/NeverSleep/MiquMaid-v2-2x70B-DPO">[DPO]</a></p>
                    <p style="padding-left: 25px;">MiquMaid-v2-2x70B is really heavy, it's a 125B model made of MiquMaid-v2-70B and Mistral Medium base. Each MoE model have 2 expert active in them, so the idea between this was to have, on every token, 2x70B expert working together for more randomness and better precision. Since MiquMaid-v2 is here only for RP, it lose some IQ point, that's where Mistral Medium hit and make the prose a lot more better, and let the model be more logical. It's better than a frakenmerge of 2 Mistral medium because it's not 1:1 2x the same model.
                    <br/>
                    The gem tho, is the DPO version. When the qLora for uncensoring was made on MiquMaid-v2 for uncensoring it, we got the idea to apply it to Mistral Medium base too. At first, the result wasn't really good and that was expected, since the qLora wasn't trained on Mistral Medium, but on MiquMaid-v2. BUT! When merged together (in a MoE for this example) the full potential of a double DPO shine.
                    <br/>
                    We got astonish result on the worst quant ever : Q2_K. Even at Q2_K, MiquMaid-v2-2x70B-DPO showed godly performance in RP, following card, logic and smut. Only downside was the repetition of the formatting, but it was really usable. Unquanted is a gem, but we doubt anyone have the compute power to do that...</p>
                </ul>
            </ul>
            <br>
            <p>(based on 0.4)</p>
            <ul>
                <p id="Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss">- <a href="https://huggingface.co/NeverSleep/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss">Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss</a></p>
                <p style="padding-left: 25px;">It's Noromaid-v0.1-mixtral-8x7b-Instruct-v3 but retrained with ChatML, Zloss(Thanks charles), and some added datasets.</p>    
                <p id="FlatOrcamaid-13b-v0.2">- <a href="https://huggingface.co/NeverSleep/FlatOrcamaid-13b-v0.2">FlatOrcamaid-13b-v0.2</a></p>
                <p style="padding-left: 25px;">FlatOrcamaid-13b-v0.2 is a merge between FlatOrca and Noromaid-13b-v0.2, see more info on the <a href="https://huggingface.co/NeverSleep/FlatOrcamaid-13b-v0.2">repo</a></p>
                <ul style="padding-left: 25px;">
                    <p style="padding-left: 25px;"><span style="color: rgb(255, 245, 110);">Important info to a 7b model:</span> Currently not possible due to Orca-2 being a LLAMA 2 7b model and Noromaid-7b being a mistral model!</p>
                    <p style="padding-left: 25px;"><span style="color: rgb(255, 245, 110);">Important info to this model:</span> This model is in NO way affiliated with <a href="https://huggingface.co/ddh0/OrcaMaid-13b">ddh0/OrcaMaid-13b</a> or and *maid models by <a href="https://huggingface.co/ddh0/OrcaMaid-13b">ddh0</a>, it was inspired by him tho.</p>
                </ul>
            </ul>
        </ul>
        
        <br>

        <p>Credits:</p>
        <ul style="padding-left: 25px;">
            <p>- Undi / Wrote parts of the blog post.</p>
        </ul>

        <br><h2>Updates:</h2>
        <div id="updates">
            <br><i class="timestamp">20.12.2023 - 11:55 [GMT+2]</i>
            <p>Okay so, because some people asked about if the datasets will be released..</p>
            <p>Yes they probably will, but not from me neither Undi. The maid part aka. Aesir will only be released after the official Aesir model from <a href="https://huggingface.co/MinervaAI">MinervaAI</a> is released.</p>
            <br><i class="timestamp">21.12.2023 - 19:56 [GMT+2]</i>
            <p>Added FlatOrcamaid-13b-v0.2 [ Released on: 20.12.2023 ]</p>
            <p>Added Noromaid-7b-v0.2 [ Released on: 21.12.2023 ]</p>
            <br><i class="timestamp">23.12.2023 - 15:03 [GMT+2]</i>
            <p>Added Noromaid-v0.1-mixtral-8x7b [ Released on: 22.12.2023 ]</p>
            <br><i class="timestamp">24.12.2023 - 13:14 [GMT+2]</i>
            <p>Added FlatOrcamaid-13b-v0.2 disclaimer</p>
            <p>Added Noromaid-v0.1-mixtral-8x7b-v2 [ Released on: 23.12.2023 ]</p>
            <p>Added Noromaid-v0.1-mixtral-8x7b-v3 [ Released on: 24.12.2023 ]</p>
            <br><i class="timestamp">26.12.2023 - 00:18 [GMT+2]</i>
            <p>Added Noromaid-v0.1-mixtral-8x7b-Instruct-v3 [ Released on: 25.12.2023 ]</p>
            <br><i class="timestamp">09.01.2024 - 20:13 [GMT+2]</i>
            <p>Added NeverSleep Discord link</p>
            <p>Added Noromaid 0.3 [ Released on: 05.01.2024 ]</p>
            <br><i class="timestamp">12.01.2024 - 19:13 [GMT+2]</i>
            <p>Added Noromaid 0.4 [ Released on: 11.01.2024 ]</p>
            <p>Added Noromaid 0.4 DPO [ Released on: 11.01.2024 ]</p>
            <p>Added credits</p>
            <p>Added Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss [ Released on: 09.01.2024 ]</p>
            <br><i class="timestamp">12.01.2024 - 23:45 [GMT+2]</i>
            <p>Did some re-formatting.</p>
            <p>Fixed some skill issue mistakes on my side.</p>
            <br><i class="timestamp">05.02.2024 - 20:14 [GMT+2]</i>
            <p>Added MiquMaid-v1-70B [ Released on: 31.01.2024 ]</p>
            <p>Added MiquMaid-v2-70B-alpha-GGUF [ Released on: 04.02.2024 ]</p>
            <br><i class="timestamp">07.02.2024 - 20:20 [GMT+2]</i>
            <p>Added MiquMaid HF Collection</p>
            <p>Added MiquMaid-v2-70B  [ Released on: 07.02.2024 ]</p>
            <p>Added MiquMaid-v2-70B-DPO  [ Released on: 07.02.2024 ]</p>
            <p>Added MiquMaid-v2-2x70B  [ Released on: 07.02.2024 ]</p>
            <p>Added MiquMaid-v2-2x70B-DPO  [ Released on: 07.02.2024 ]</p>
        </div>
    </div>
    <div id="line"></div>
</div>

<div>
    <div class="blogbox" id="blogid-5">
        <h3>Collab with Undi / Echidna-13b v0.1, v0.2, v0.3 + NeverSleep/Nethena-13B</h3>
        <i class="timestamp">27.10.2023 - 19:57 - 23:53 [GMT+2]</i><div style="padding-top: 20px"></div>
        <h4><a href="https://huggingface.co/NeverSleep/Echidna-13b-v0.1">Echidna-13b-v0.1</a></h4>
        <p>Echidna-13b-v0.1 came out 21.10.2023, i wanted to merge it alone first, but had server issues. Thats when i asked Undi to help merge it, we reviewed my recipe and decided to upload it on his <a href="https://huggingface.co/NeverSleep">NeverSleep</a> org and let it be a collab.</p>
        <h4><a href="https://huggingface.co/NeverSleep/Echidna-13b-v0.2">Echidna-13b-v0.2</a></h4>
        <p>Echidna-13b-v0.2 came out 26.10.2023 (took 4~ hours of merging), it was kind of a "test" version of Echidna-13b-v0.3. Echidna-13b-v0.3 would follow soon after.</p>
        <h4><a href="https://huggingface.co/NeverSleep/Echidna-13b-v0.3">Echidna-13b-v0.3</a></h4>
        <p>It took us some time to refine <span class="addspeech addspeechstyle" title="You can look at the recipe on the HF/Modelcard/Readme">Echidna-13b-v0.2's recipe</span>, we had multiple test models(private) which game way better results than Echidna-13b-v0.2.</p>
        <p>The newest version, Echidna-13b-v0.2 came out 27.10.2023 (took 5.30~ hours of merging).</p>
        <p>Repetition seems to be almost non-existant, which is really nice. Undi and me noticed that while trying to let the AI talk to itself(with impersonate).</p>
        <p>Coherency is outstanding, as is creativity.</p>
        <p>This model seems to not have 5 IQ like some other models, for example: a femboy has a dick, not a vagina... It's also really nice at describing things, like scenes. This is probably due to having so much medical data in here.</p>
        <p>Last but not least.. not a single <span class="addspeech rainbow_text_animated" title="<span class='rainbow_text_animated'>Ministrations</span>">"...shiver down X spine..."</span> seen while testing!</p>

        <br><h2>Updates:</h2>
        <div id="updates">
            <br><i class="timestamp">31.10.2023 - 01:50 [GMT+2]</i><div style="padding-top: 20px"></div>
            <h4><a href="https://huggingface.co/NeverSleep/Nethena-13B">Nethena-13B</a></h4>
            <p>Tbh i cant really keep up with how much models we are posting on here kek..</p>
            <p>Well anyways... we released a model called Nethena yesterday which is even better than echidna, this model combines every best model of Undi and me:</p>
            <p>Athena v3 + Nete + Echidna v0.3</p>
            <br>
            <p>We also made a <a href="https://huggingface.co/NeverSleep/Nethena-20B">Nethena-20B</a> but no one really cares about that.</p>
            <h4 class="rainbow_text_animated" style="font-size: 20px;">Please enjoy!</h4>
        </div>
    </div>
    <div id="line"></div>
</div>

<div>
    <div class="blogbox" id="blogid-4">
        <h3>MinervaAI / Aesir</h3>
        <i class="timestamp">16.09.2023 - 23:31 [GMT+2]</i><div style="padding-top: 20px"></div>
        <p>Sooo.. hey.. me, Gryphepadar, Doctorshotgun, Lordgoonery, M1xx3l, Nruaif are in sort of(?) a little team making LLM models no.</p>
        <p>Currently we are working on a model + dataset called Aesir, we are almost 1 dataset manual cleaning down.</p>
        <p>Aesir is meant as a horny-ass RP model(ERP).</p>
        <p>You can get more info on our <a href="https://huggingface.co/MinervaAI">HuggingFace</a>.</p>
    </div>
    <div id="line"></div>
</div>

<div>
    <div class="blogbox" id="blogid-3">
        <h3>Wanna check out my resources page?</h3>
        <i class="timestamp">14.08.2023 - 23:17 [GMT+2]</i><div style="padding-top: 20px"></div>
        <p>So i added a little section called "Resources", if you want you can check it out if you wanna look for new stuff programming/ML/LLM/SD or if you are starting out with that stuff.</p>
        <p>You can check it out <a onclick="switchTo(3)">here</a> or you can just click the "resources" button in the Navbar.</p>
        <br>
        <p>One thing i still need to do tho, is making this website better for mobile, as it looks shit currently(i think so at least).</p>
        
        <br><h2>Updates:</h2>
        <div id="updates">
            <i class="timestamp">15.08.2023 - 00:49 [GMT+2]</i><div style="padding-top: 20px"></div>
            <p>I also added a projects page, where i will post future projects.</p>
        </div>
    </div>
    <div id="line"></div>
</div>


<div>
    <div class="blogbox" id="blogid-2">
        <h3>Website is finally "done"</h3>
        <i class="timestamp">13.08.2023 - 22:29 [GMT+2]</i><div style="padding-top: 20px"></div>
        <p>Hey, i finally made the website here have content, and if you are asking "ikari, why tf are you posting so many blog posts, aren't they suposed to be kind of 'special'?"</p>
        <p>To that i answer "Shut up, i need to fill this page so it doesn't look empty!"</p>
        <p>Anyway.. it now looks awesome! And i removed all the lorem impsum shit, + I added, if you open a post in "full screen" it temporarily removes the sidebar for better visibility of the post.</p>
        
        <br><h2>Updates:</h2>
        <div id="updates">
            <i class="timestamp">14.08.2023 - 00:39 [GMT+2]</i><div style="padding-top: 20px"></div>
            <p>I added a nice looking loading screen, to hide the js magic, and i fixed a couple of things, + quality of life stuff.</p>
            <p>One cool thing i added too, was this vote thingy, if you like a post you can vote it up! (you can for some reason press it multiple times, so if you REALLY like a post you could vote it to like 200 or smth)</p>
        </div>
    </div>
    <div id="line"></div>
</div>

<div>
    <div class="blogbox" id="blogid-1">
        <h3>So i added a couple of cool things</h3>
        <i class="timestamp">12.08.2023 - 23:05 [GMT+2]</i><div style="padding-top: 20px"></div>
        <p>I added a new url arg called "bo", when set to true combined with set blogid arg, it opens it in "fullscreen" mode.</p>
        <p>I also added a little button(Open post), which does the same. When you click share it automatically sets the blogid arg, + it sets "bo" arg to true.</p>
    </div>
    <div id="line"></div>
</div>


<div>
    <div class="blogbox" id="blogid-0">
        <h3>Hey, this is a little test for the new blog page</h3>
        <i class="timestamp">12.08.2023 - 04:15 [GMT+2]</i><div style="padding-top: 20px"></div>
        <p>I will document my jurney as a dev here.</p>
        <p>I dont think anyone is even reading this but yea, i dont really care xD</p>
        <p>If you wanna for some reason share this or other posts, i added that thing below</p>
    </div>
    <div id="line"></div>
</div>
